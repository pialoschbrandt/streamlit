{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b19a4c",
   "metadata": {},
   "source": [
    "## Lenker til prosjektet\n",
    "\n",
    "- GitHub-repositorium: https://github.com/pialoschbrandt/streamlit#\n",
    "- Streamlit-app: https://appgit-2khm3anafqsdgqrdfpx7vz.streamlit.app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491936d5",
   "metadata": {},
   "source": [
    "https://appgit-2khm3anafqsdgqrdfpx7vz.streamlit.app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b3b2c",
   "metadata": {},
   "source": [
    "# LOGG 4\n",
    "\n",
    "\n",
    "## 2.1 Correlation slider \n",
    "\n",
    "After adjusting the slider and examining the correlation with 24-hour and 72-hour time shifts, the findings indicate that the correlation between weather variables, seasonality, and energy consumption is generally weak and appears somewhat inconsistent. When analyzing shorter intervals, slightly clearer patterns can be observed, whereas longer intervals tend to smooth out variations and reduce visible correlations. There is, however, a faint indication of a relationship between consumption and high temperatures during the summer months.\n",
    "These observations highlight the importance of having multiple energy sources, as relying on a single weather-dependent factor would not be sufficient to maintain stable energy availability.\n",
    "## 3.1 Implimentation of bonus task\n",
    "\n",
    "The bonus task I chose to implement focuses on improving the app‚Äôs user experience and robustness. Since several functions in the application take a noticeable amount of time to run‚Äîespecially the one that retrieves data from MongoDB‚ÄîI implemented spinners to clearly indicate that the app is working and that a process is ongoing. This helps prevent users from thinking that the application has frozen.\n",
    "I also spent time improving error handling throughout the code. Instead of allowing the app to crash or display cryptic error messages, I added mechanisms to catch errors and provide meaningful, user-friendly feedback.\n",
    "## 4.1 - App Structure and Navigation\n",
    "\n",
    "The app is organized with a clear structure so that users can easily find what they need without being overwhelmed. All features that belong together are grouped into their own sections. For example, everything related to weather data is placed in one area, while geo and snow drift tools are placed in another. More advanced analytical tools, such as SPC, LOF, and sliding correlations, also have their own section. \n",
    "\n",
    "\n",
    "This makes the app easier to understand and keeps the content logically categorized.\n",
    "To avoid showing too much information at once, the app uses subpages, where each analysis is placed on its own page. The home screen provides an overview of what the dashboard contains, with short descriptions of the different categories. This helps the user understand the structure before exploring the details.\n",
    "\n",
    "\n",
    "\n",
    "Navigation happens mainly through the sidebar, which acts as the app‚Äôs menu. Each page also includes simple explanations and short descriptions of how to use the tools and how to interpret the plots. This makes the dashboard more user-friendly and ensures that the user always knows what they are looking at and how the analysis works.\n",
    "## 5.1 - Problems\n",
    "\n",
    "\n",
    "I experienced issues when trying to push data to Cassandra, for reasons that were not fully clear. A more detailed description of this problem is included in the notebook ind320_2.ipynb. Because of this, I was not able to complete the Cassandra part as intended, and instead I pushed the data directly to MongoDB from the API.\n",
    "\n",
    "\n",
    "The day before the submission, I also ran into another issue: my MongoDB storage limit was exceeded. As a result, I had to create a new MongoDB user and reload all the data into a fresh database. I then updated my script so that the app retrieved the data from this new database. Despite these challenges, the data pipeline and dashboard now run correctly using MongoDB.\n",
    "## 6.1 - How was AI used? \n",
    "\n",
    "AI was used to translate and rewrite code, comments, and documentation into English.\n",
    "It provided explanations of advanced concepts and guided me through debugging and troubleshooting issues in the project.\n",
    "AI also helped improve the structure and clarity of the documentation, including updates to README.md and requirements.txt.\n",
    "In addition, it supported rewriting text sections for better readability and consistency throughout the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a560106",
   "metadata": {},
   "source": [
    " ## 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4932b6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pialoschbrandt/anaconda3/envs/streamlit/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import date, timedelta\n",
    "import calendar\n",
    "import os\n",
    "import sys\n",
    "from pymongo.mongo_client import MongoClient    \n",
    "from pymongo.server_api import ServerApi\n",
    "import plotly.express as px\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce089c6",
   "metadata": {},
   "source": [
    "## 1.2 Push data to MongoDB (2021-2024)\n",
    "- Pushing data to Cassandra is skipt due to same problem as in describe in the Jupyter Notebook Ind320_2.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a875fb0",
   "metadata": {},
   "source": [
    "### PRODUCTION_PER_GROUP_MBA_HOUR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0739a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching January 2021 ... OK\n",
      "Fetching February 2021 ... OK\n",
      "Fetching March 2021 ... OK\n",
      "Fetching April 2021 ... OK\n",
      "Fetching May 2021 ... OK\n",
      "Fetching June 2021 ... OK\n",
      "Fetching July 2021 ... OK\n",
      "Fetching August 2021 ... OK\n",
      "Fetching September 2021 ... OK\n",
      "Fetching October 2021 ... OK\n",
      "Fetching November 2021 ... OK\n",
      "Fetching December 2021 ... OK\n",
      "Fetching January 2022 ... OK\n",
      "Fetching February 2022 ... OK\n",
      "Fetching March 2022 ... OK\n",
      "Fetching April 2022 ... OK\n",
      "Fetching May 2022 ... OK\n",
      "Fetching June 2022 ... OK\n",
      "Fetching July 2022 ... OK\n",
      "Fetching August 2022 ... OK\n",
      "Fetching September 2022 ... OK\n",
      "Fetching October 2022 ... OK\n",
      "Fetching November 2022 ... OK\n",
      "Fetching December 2022 ... OK\n",
      "Fetching January 2023 ... OK\n",
      "Fetching February 2023 ... OK\n",
      "Fetching March 2023 ... OK\n",
      "Fetching April 2023 ... OK\n",
      "Fetching May 2023 ... OK\n",
      "Fetching June 2023 ... OK\n",
      "Fetching July 2023 ... OK\n",
      "Fetching August 2023 ... OK\n",
      "Fetching September 2023 ... OK\n",
      "Fetching October 2023 ... OK\n",
      "Fetching November 2023 ... OK\n",
      "Fetching December 2023 ... OK\n",
      "Fetching January 2024 ... OK\n",
      "Fetching February 2024 ... OK\n",
      "Fetching March 2024 ... OK\n",
      "Fetching April 2024 ... OK\n",
      "Fetching May 2024 ... OK\n",
      "Fetching June 2024 ... OK\n",
      "Fetching July 2024 ... OK\n",
      "Fetching August 2024 ... OK\n",
      "Fetching September 2024 ... OK\n",
      "Fetching October 2024 ... OK\n",
      "Fetching November 2024 ... OK\n",
      "Fetching December 2024 ... OK\n",
      "\n",
      "Done fetching data.\n",
      "Total rows: 872953\n",
      "  country priceArea productionGroup  quantityKwh                  startTime  \\\n",
      "0      NO       NO1           hydro    2507716.8  2021-01-01T00:00:00+01:00   \n",
      "1      NO       NO1           hydro    2494728.0  2021-01-01T01:00:00+01:00   \n",
      "2      NO       NO1           hydro    2486777.5  2021-01-01T02:00:00+01:00   \n",
      "3      NO       NO1           hydro    2461176.0  2021-01-01T03:00:00+01:00   \n",
      "4      NO       NO1           hydro    2466969.2  2021-01-01T04:00:00+01:00   \n",
      "\n",
      "                     endTime            lastUpdatedTime  \n",
      "0  2021-01-01T01:00:00+01:00  2024-12-20T10:35:40+01:00  \n",
      "1  2021-01-01T02:00:00+01:00  2024-12-20T10:35:40+01:00  \n",
      "2  2021-01-01T03:00:00+01:00  2024-12-20T10:35:40+01:00  \n",
      "3  2021-01-01T04:00:00+01:00  2024-12-20T10:35:40+01:00  \n",
      "4  2021-01-01T05:00:00+01:00  2024-12-20T10:35:40+01:00  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "\n",
    "# URL for Elhub's energy data API\n",
    "base_url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "\n",
    "# Parameters for the API request ‚Äì defines which dataset to fetch\n",
    "params = {'dataset': 'PRODUCTION_PER_GROUP_MBA_HOUR'}\n",
    "\n",
    "# List to collect all DataFrames\n",
    "all_data = []\n",
    "\n",
    "# Loop through all years 2021‚Äì2024\n",
    "for year in range(2021, 2025):\n",
    "    for month in range(1, 13):\n",
    "\n",
    "        # Start and end of month\n",
    "        start = date(year, month, 1)\n",
    "        end = date(year + 1, 1, 1) if month == 12 else date(year, month + 1, 1)\n",
    "\n",
    "        # Add timezone to match your original version\n",
    "        params['startDate'] = f\"{start.isoformat()}T00:00:00+02:00\"\n",
    "        params['endDate']   = f\"{end.isoformat()}T00:00:00+02:00\"\n",
    "\n",
    "        print(f\"Fetching {start.strftime('%B %Y')} ...\", end=\" \")\n",
    "\n",
    "        # API request\n",
    "        r = requests.get(base_url, params=params)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        data = r.json().get('data', [])\n",
    "\n",
    "        # Build rows\n",
    "        rows = []\n",
    "        for d in data:\n",
    "            attr = d['attributes']\n",
    "            for p in attr['productionPerGroupMbaHour']:\n",
    "                rows.append({\n",
    "                    'country': attr.get('country'),\n",
    "                    'priceArea': p.get('priceArea'),\n",
    "                    'productionGroup': p.get('productionGroup'),\n",
    "                    'quantityKwh': p.get('quantityKwh'),\n",
    "                    'startTime': p.get('startTime'),\n",
    "                    'endTime': p.get('endTime'),\n",
    "                    'lastUpdatedTime': p.get('lastUpdatedTime')\n",
    "                })\n",
    "\n",
    "        if rows:\n",
    "            all_data.append(pd.DataFrame(rows))\n",
    "            print(\"OK\")\n",
    "        else:\n",
    "            print(\"EMPTY\")\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "df_all = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "print(\"\\nDone fetching data.\")\n",
    "print(\"Total rows:\", len(df_all))\n",
    "print(df_all.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1119142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F√∏rste dato: 2020-12-31 23:00:00+00:00\n",
      "Siste dato: 2024-12-31 22:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "#Sjekker f√∏rste og siste dato i df_all for √• sjekka at alle √∏nskede datoer er med\n",
    "df_all[\"startTime\"] = pd.to_datetime(df_all[\"startTime\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "print(\"F√∏rste dato:\", df_all[\"startTime\"].min())\n",
    "print(\"Siste dato:\", df_all[\"startTime\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd9f420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021 2022 2023 2024]\n"
     ]
    }
   ],
   "source": [
    "#fjerner datoer fra 2020\n",
    "df_all[\"endTime\"] = pd.to_datetime(df_all[\"endTime\"], utc=True, errors=\"coerce\")\n",
    "df_all = df_all[df_all[\"endTime\"].dt.year >= 2021]\n",
    "print(df_all[\"endTime\"].dt.year.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad1020ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 872953\n",
      "\n",
      "Column names in df_all:\n",
      "['country', 'priceArea', 'productionGroup', 'quantityKwh', 'startTime', 'endTime', 'lastUpdatedTime']\n",
      "\n",
      "Columns:\n",
      "- country\n",
      "- priceArea\n",
      "- productionGroup\n",
      "- quantityKwh\n",
      "- startTime\n",
      "- endTime\n",
      "- lastUpdatedTime\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of rows in the DataFrame\n",
    "print(\"Number of rows:\", len(df_all))\n",
    "\n",
    "# ---- Get column names ----\n",
    "print(\"\\nColumn names in df_all:\")\n",
    "print(list(df_all.columns))\n",
    "\n",
    "# More readable with line breaks:\n",
    "print(\"\\nColumns:\")\n",
    "for col in df_all.columns:\n",
    "    print(\"-\", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "893c3da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  price_area production_group                start_time  \\\n",
      "0        NO1            hydro 2020-12-31 23:00:00+00:00   \n",
      "1        NO1            hydro 2021-01-01 00:00:00+00:00   \n",
      "2        NO1            hydro 2021-01-01 01:00:00+00:00   \n",
      "3        NO1            hydro 2021-01-01 02:00:00+00:00   \n",
      "4        NO1            hydro 2021-01-01 03:00:00+00:00   \n",
      "\n",
      "                   end_time  quantity_kwh  \n",
      "0 2021-01-01 00:00:00+00:00     2507716.8  \n",
      "1 2021-01-01 01:00:00+00:00     2494728.0  \n",
      "2 2021-01-01 02:00:00+00:00     2486777.5  \n",
      "3 2021-01-01 03:00:00+00:00     2461176.0  \n",
      "4 2021-01-01 04:00:00+00:00     2466969.2  \n",
      "Columns after rename: ['price_area', 'production_group', 'start_time', 'end_time', 'quantity_kwh']\n"
     ]
    }
   ],
   "source": [
    "# Prepare the DataFrame before converting it\n",
    "# Keep only the relevant columns\n",
    "df_ready = df_all[[\"priceArea\", \"productionGroup\", \"startTime\", \"endTime\", \"quantityKwh\"]].copy()\n",
    "\n",
    "# Rename columns to snake_case\n",
    "df_ready.rename(columns={\n",
    "    \"priceArea\": \"price_area\",\n",
    "    \"productionGroup\": \"production_group\",\n",
    "    \"startTime\": \"start_time\",\n",
    "    \"endTime\": \"end_time\",\n",
    "    \"quantityKwh\": \"quantity_kwh\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Remove any rows missing key values\n",
    "df_ready.dropna(subset=[\"price_area\", \"start_time\"], inplace=True)\n",
    "\n",
    "# Convert time columns from string to datetime with UTC\n",
    "df_ready[\"start_time\"] = pd.to_datetime(df_ready[\"start_time\"], utc=True, errors=\"coerce\")\n",
    "df_ready[\"end_time\"] = pd.to_datetime(df_ready[\"end_time\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "print(df_ready.head())\n",
    "print(\"Columns after rename:\", list(df_ready.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c58dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "USR, PWD = open('../no_sync/mongo_db.txt').read().splitlines()\n",
    "\n",
    "uri = f\"mongodb+srv://{USR}:{PWD}@cluster0.b9pfgmh.mongodb.net/\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf617727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Slettet 0 dokumenter fra collectionen.\n"
     ]
    }
   ],
   "source": [
    "# Velg database og collection\n",
    "database = client['elhub_data']\n",
    "collection = database['production_per_group_mba_hour'] \n",
    "\n",
    "# ‚ùó Slett alle dokumenter i collection (men behold strukturen)\n",
    "delete_result = collection.delete_many({})\n",
    "print(f\"üóëÔ∏è  Slettet {delete_result.deleted_count} dokumenter fra collectionen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bc56847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Elhub-data er lastet opp til MongoDB (f√∏rste gang).\n"
     ]
    }
   ],
   "source": [
    "# Sjekk om collection allerede inneholder data\n",
    "existing_count = collection.count_documents({})\n",
    "\n",
    "if existing_count <= 1:\n",
    "    data = df_all.to_dict(\"records\")\n",
    "    collection.insert_many(df_ready.to_dict(\"records\"))\n",
    "    print(\"‚úÖ Elhub-data er lastet opp til MongoDB (f√∏rste gang).\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Collection inneholder allerede {existing_count} dokumenter ‚Äî hopper over opplasting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce3d700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in 'production_per_group_mba_hour': 872953\n"
     ]
    }
   ],
   "source": [
    "# Select the database and collection\n",
    "database = client['elhub_data']\n",
    "collection = database['production_per_group_mba_hour']\n",
    "\n",
    "# Count the number of documents (rows)\n",
    "num_docs = collection.count_documents({})\n",
    "print(f\"Number of documents in 'production_per_group_mba_hour': {num_docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06035d7",
   "metadata": {},
   "source": [
    "### CONSUMPTION_PER_GROUP_MBA_HOUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce6ca690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching consumption January 2021 ... OK\n",
      "Fetching consumption February 2021 ... OK\n",
      "Fetching consumption March 2021 ... OK\n",
      "Fetching consumption April 2021 ... OK\n",
      "Fetching consumption May 2021 ... OK\n",
      "Fetching consumption June 2021 ... OK\n",
      "Fetching consumption July 2021 ... OK\n",
      "Fetching consumption August 2021 ... OK\n",
      "Fetching consumption September 2021 ... OK\n",
      "Fetching consumption October 2021 ... OK\n",
      "Fetching consumption November 2021 ... OK\n",
      "Fetching consumption December 2021 ... OK\n",
      "Fetching consumption January 2022 ... OK\n",
      "Fetching consumption February 2022 ... OK\n",
      "Fetching consumption March 2022 ... OK\n",
      "Fetching consumption April 2022 ... OK\n",
      "Fetching consumption May 2022 ... OK\n",
      "Fetching consumption June 2022 ... OK\n",
      "Fetching consumption July 2022 ... OK\n",
      "Fetching consumption August 2022 ... OK\n",
      "Fetching consumption September 2022 ... OK\n",
      "Fetching consumption October 2022 ... OK\n",
      "Fetching consumption November 2022 ... OK\n",
      "Fetching consumption December 2022 ... OK\n",
      "Fetching consumption January 2023 ... OK\n",
      "Fetching consumption February 2023 ... OK\n",
      "Fetching consumption March 2023 ... OK\n",
      "Fetching consumption April 2023 ... OK\n",
      "Fetching consumption May 2023 ... OK\n",
      "Fetching consumption June 2023 ... OK\n",
      "Fetching consumption July 2023 ... OK\n",
      "Fetching consumption August 2023 ... OK\n",
      "Fetching consumption September 2023 ... OK\n",
      "Fetching consumption October 2023 ... OK\n",
      "Fetching consumption November 2023 ... OK\n",
      "Fetching consumption December 2023 ... OK\n",
      "Fetching consumption January 2024 ... OK\n",
      "Fetching consumption February 2024 ... OK\n",
      "Fetching consumption March 2024 ... OK\n",
      "Fetching consumption April 2024 ... OK\n",
      "Fetching consumption May 2024 ... OK\n",
      "Fetching consumption June 2024 ... OK\n",
      "Fetching consumption July 2024 ... OK\n",
      "Fetching consumption August 2024 ... OK\n",
      "Fetching consumption September 2024 ... OK\n",
      "Fetching consumption October 2024 ... OK\n",
      "Fetching consumption November 2024 ... OK\n",
      "Fetching consumption December 2024 ... OK\n",
      "\n",
      "Done fetching consumption data.\n",
      "Total rows: 876600\n",
      "  country priceArea consumptionGroup  quantityKwh                  startTime  \\\n",
      "0      NO       NO1            cabin    177071.56  2021-01-01T00:00:00+01:00   \n",
      "1      NO       NO1            cabin    171335.12  2021-01-01T01:00:00+01:00   \n",
      "2      NO       NO1            cabin    164912.02  2021-01-01T02:00:00+01:00   \n",
      "3      NO       NO1            cabin    160265.77  2021-01-01T03:00:00+01:00   \n",
      "4      NO       NO1            cabin    159828.69  2021-01-01T04:00:00+01:00   \n",
      "\n",
      "                     endTime            lastUpdatedTime  \n",
      "0  2021-01-01T01:00:00+01:00  2024-12-20T10:35:40+01:00  \n",
      "1  2021-01-01T02:00:00+01:00  2024-12-20T10:35:40+01:00  \n",
      "2  2021-01-01T03:00:00+01:00  2024-12-20T10:35:40+01:00  \n",
      "3  2021-01-01T04:00:00+01:00  2024-12-20T10:35:40+01:00  \n",
      "4  2021-01-01T05:00:00+01:00  2024-12-20T10:35:40+01:00  \n"
     ]
    }
   ],
   "source": [
    "# URL for Elhub API\n",
    "base_url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "\n",
    "# Parameter: consumption dataset\n",
    "params = {'dataset': 'CONSUMPTION_PER_GROUP_MBA_HOUR'}\n",
    "\n",
    "# List for storing monthly DataFrames\n",
    "all_CONS = []\n",
    "\n",
    "# Loop through all years 2021‚Äì2024\n",
    "for year in range(2021, 2025):\n",
    "    for month in range(1, 13):\n",
    "\n",
    "        # Start and end of the month\n",
    "        start = date(year, month, 1)\n",
    "        end = date(year + 1, 1, 1) if month == 12 else date(year, month + 1, 1)\n",
    "\n",
    "        params[\"startDate\"] = f\"{start.isoformat()}T00:00:00+02:00\"\n",
    "        params[\"endDate\"]   = f\"{end.isoformat()}T00:00:00+02:00\"\n",
    "\n",
    "        print(f\"Fetching consumption {start.strftime('%B %Y')} ...\", end=\" \")\n",
    "\n",
    "        # API request\n",
    "        r = requests.get(base_url, params=params)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        data_C = r.json().get(\"data\", [])\n",
    "\n",
    "        rows = []\n",
    "        for d in data_C:\n",
    "            attr = d[\"attributes\"]\n",
    "\n",
    "            # IMPORTANT: use consumptionPerGroupMbaHour\n",
    "            for p in attr[\"consumptionPerGroupMbaHour\"]:\n",
    "                rows.append({\n",
    "                    \"country\": attr.get(\"country\"),\n",
    "                    \"priceArea\": p.get(\"priceArea\"),\n",
    "                    \"consumptionGroup\": p.get(\"consumptionGroup\"),\n",
    "                    \"quantityKwh\": p.get(\"quantityKwh\"),\n",
    "                    \"startTime\": p.get(\"startTime\"),\n",
    "                    \"endTime\": p.get(\"endTime\"),\n",
    "                    \"lastUpdatedTime\": p.get(\"lastUpdatedTime\")\n",
    "                })\n",
    "\n",
    "        if rows:\n",
    "            all_CONS.append(pd.DataFrame(rows))\n",
    "            print(\"OK\")\n",
    "        else:\n",
    "            print(\"EMPTY\")\n",
    "\n",
    "# Combine all consumption data\n",
    "df_consumption = pd.concat(all_CONS, ignore_index=True)\n",
    "\n",
    "print(\"\\nDone fetching consumption data.\")\n",
    "print(\"Total rows:\", len(df_consumption))\n",
    "print(df_consumption.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95a195ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F√∏rste dato: 2020-12-31 23:00:00+00:00\n",
      "Siste dato: 2024-12-31 22:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "#Sjekker f√∏rste og siste dato i df_all for √• sjekka at alle √∏nskede datoer er med\n",
    "df_consumption[\"startTime\"] = pd.to_datetime(df_consumption[\"startTime\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "print(\"F√∏rste dato:\", df_consumption[\"startTime\"].min())\n",
    "print(\"Siste dato:\", df_consumption[\"startTime\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d00135ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021 2022 2023 2024]\n"
     ]
    }
   ],
   "source": [
    "#fjerner datoer fra 2020\n",
    "df_consumption[\"endTime\"] = pd.to_datetime(df_consumption[\"endTime\"], utc=True, errors=\"coerce\")\n",
    "df_consumption = df_consumption[df_consumption[\"endTime\"].dt.year >= 2021]\n",
    "print(df_consumption[\"endTime\"].dt.year.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04837864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['country', 'priceArea', 'consumptionGroup', 'quantityKwh', 'startTime',\n",
      "       'endTime', 'lastUpdatedTime'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_consumption.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "076d6899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country price_area consumption_group  quantity_kwh  \\\n",
      "0      NO        NO1             cabin     177071.56   \n",
      "1      NO        NO1             cabin     171335.12   \n",
      "2      NO        NO1             cabin     164912.02   \n",
      "3      NO        NO1             cabin     160265.77   \n",
      "4      NO        NO1             cabin     159828.69   \n",
      "\n",
      "                 start_time                  end_time  \\\n",
      "0 2020-12-31 23:00:00+00:00 2021-01-01 00:00:00+00:00   \n",
      "1 2021-01-01 00:00:00+00:00 2021-01-01 01:00:00+00:00   \n",
      "2 2021-01-01 01:00:00+00:00 2021-01-01 02:00:00+00:00   \n",
      "3 2021-01-01 02:00:00+00:00 2021-01-01 03:00:00+00:00   \n",
      "4 2021-01-01 03:00:00+00:00 2021-01-01 04:00:00+00:00   \n",
      "\n",
      "          last_updated_time  \n",
      "0 2024-12-20 09:35:40+00:00  \n",
      "1 2024-12-20 09:35:40+00:00  \n",
      "2 2024-12-20 09:35:40+00:00  \n",
      "3 2024-12-20 09:35:40+00:00  \n",
      "4 2024-12-20 09:35:40+00:00  \n",
      "Columns after cleaning: ['country', 'price_area', 'consumption_group', 'quantity_kwh', 'start_time', 'end_time', 'last_updated_time']\n",
      "Total rows after cleaning: 876600\n"
     ]
    }
   ],
   "source": [
    "# Extract only the columns that actually exist\n",
    "df_cons_ready = df_consumption[[\n",
    "    \"country\",\n",
    "    \"priceArea\",\n",
    "    \"consumptionGroup\",\n",
    "    \"quantityKwh\",\n",
    "    \"startTime\",\n",
    "    \"endTime\",\n",
    "    \"lastUpdatedTime\"\n",
    "]].copy()\n",
    "\n",
    "# Rename to snake_case for consistency\n",
    "df_cons_ready.rename(columns={\n",
    "    \"country\": \"country\",\n",
    "    \"priceArea\": \"price_area\",\n",
    "    \"consumptionGroup\": \"consumption_group\",\n",
    "    \"quantityKwh\": \"quantity_kwh\",\n",
    "    \"startTime\": \"start_time\",\n",
    "    \"endTime\": \"end_time\",\n",
    "    \"lastUpdatedTime\": \"last_updated_time\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Drop rows missing essential values\n",
    "df_cons_ready.dropna(subset=[\"price_area\", \"start_time\"], inplace=True)\n",
    "\n",
    "# Convert time columns to proper datetime format with UTC\n",
    "df_cons_ready[\"start_time\"] = pd.to_datetime(df_cons_ready[\"start_time\"], utc=True, errors=\"coerce\")\n",
    "df_cons_ready[\"end_time\"]   = pd.to_datetime(df_cons_ready[\"end_time\"],   utc=True, errors=\"coerce\")\n",
    "df_cons_ready[\"last_updated_time\"] = pd.to_datetime(df_cons_ready[\"last_updated_time\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "print(df_cons_ready.head())\n",
    "print(\"Columns after cleaning:\", list(df_cons_ready.columns))\n",
    "print(\"Total rows after cleaning:\", len(df_cons_ready))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "247c8ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Slettet 0 dokumenter fra collectionen.\n",
      "‚úÖ Lastet opp 876600 konsum-dokumenter!\n"
     ]
    }
   ],
   "source": [
    "# 1) Velg database og COLLECTION for konsum\n",
    "database = client['elhub_data']\n",
    "collection = database['consumption_per_group_mba_hour']\n",
    "\n",
    "# 2) T√∏m collection hvis den finnes\n",
    "delete_result = collection.delete_many({})\n",
    "print(f\"üßπ Slettet {delete_result.deleted_count} dokumenter fra collectionen.\")\n",
    "\n",
    "# 3) Sjekk om collection er tom og last opp\n",
    "existing_count = collection.count_documents({})\n",
    "\n",
    "if existing_count == 0:\n",
    "    collection.insert_many(df_cons_ready.to_dict(\"records\"))\n",
    "    print(f\"‚úÖ Lastet opp {len(df_cons_ready)} konsum-dokumenter!\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Collection inneholder allerede {existing_count} dokumenter ‚Äî hopper over opplasting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b0f8b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in 'consumption_per_group_mba_hour': 876600\n"
     ]
    }
   ],
   "source": [
    "# Select the database and collection\n",
    "database = client['elhub_data']\n",
    "collection = database['consumption_per_group_mba_hour']\n",
    "\n",
    "# Count the number of documents (rows)\n",
    "num_doc = collection.count_documents({})\n",
    "print(f\"Number of documents in 'consumption_per_group_mba_hour': {num_doc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (streamlit)",
   "language": "python",
   "name": "streamlit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
